{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116cd1bd",
   "metadata": {},
   "source": [
    "# Feature Engineering Jupyter Notebook\n",
    "## Objectives\n",
    "- Engineer features to predict SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d13a4",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine import transformation as vt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "import ppscore as pps\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e73eb4",
   "metadata": {},
   "source": [
    "### Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"outputs/Cleaned.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7de4be",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial data types:\\n\", df.dtypes)\n",
    "print(\"Initial data preview:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a07fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Store'] = df['Store'].astype('object')\n",
    "df['Dept'] = df['Dept'].astype('int64')\n",
    "df['Dept'] = df['Dept'].astype('object')\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5b270",
   "metadata": {},
   "source": [
    "### Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
    "                    linewidth=0.5\n",
    "                    )\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "\n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
    "                         linewidth=0.05, linecolor='grey')\n",
    "\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    df_corr_spearman = df.corr(method=\"spearman\")\n",
    "    df_corr_pearson = df.corr(method=\"pearson\")\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(\n",
    "        columns='x', index='y', values='ppscore')\n",
    "\n",
    "    pps_score_stats = pps_matrix_raw.query(\n",
    "        \"ppscore < 1\").filter(['ppscore']).describe().T\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(20, 12), font_annot=8):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "    print(\"* Analyse multi collinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold,\n",
    "                 figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold,\n",
    "                 figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold,\n",
    "                figsize=figsize, font_annot=font_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d061602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCorrAndPPS(df_corr_pearson=df_corr_pearson,\n",
    "                  df_corr_spearman=df_corr_spearman,\n",
    "                  pps_matrix=pps_matrix,\n",
    "                  CorrThreshold=0.05, PPS_Threshold=0.1,\n",
    "                  figsize=(20, 15), font_annot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c1852",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
    "    \"\"\"\n",
    "    - used for quick feature engineering on numerical and categorical variables\n",
    "    to decide which transformation can better transform the distribution shape \n",
    "    - Once transformed, use a reporting tool, like ydata-profiling, to evaluate distributions\n",
    "\n",
    "    \"\"\"\n",
    "    check_missing_values(df)\n",
    "    allowed_types = ['numerical', 'onehot_encoder',  'outlier_winsorizer']\n",
    "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
    "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
    "\n",
    "    # Loop in each variable and engineer the data according to the analysis type\n",
    "    df_feat_eng = pd.DataFrame([])\n",
    "    for column in df.columns:\n",
    "        # create additional columns (column_method) to apply the methods\n",
    "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
    "        for method in list_column_transformers:\n",
    "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
    "\n",
    "        # Apply transformers in respective column_transformers\n",
    "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
    "            analysis_type, df_feat_eng, column)\n",
    "\n",
    "        # For each variable, assess how the transformations perform\n",
    "        transformer_evaluation(\n",
    "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
    "\n",
    "    return df_feat_eng\n",
    "\n",
    "\n",
    "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
    "    # Check analysis type\n",
    "    if analysis_type == None:\n",
    "        raise SystemExit(\n",
    "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
    "    if analysis_type not in allowed_types:\n",
    "        raise SystemExit(\n",
    "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
    "\n",
    "\n",
    "def check_missing_values(df):\n",
    "    if df.isna().sum().sum() != 0:\n",
    "        raise SystemExit(\n",
    "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
    "\n",
    "\n",
    "def define_list_column_transformers(analysis_type):\n",
    "    # Set suffix columns according to analysis_type\n",
    "    if analysis_type == 'numerical':\n",
    "        list_column_transformers = [\n",
    "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
    "\n",
    "    elif analysis_type == 'onehot_encoder':\n",
    "        list_column_transformers = [\"onehot_encoder\"]\n",
    "\n",
    "    elif analysis_type == 'outlier_winsorizer':\n",
    "        list_column_transformers = ['iqr']\n",
    "\n",
    "    return list_column_transformers\n",
    "\n",
    "\n",
    "def apply_transformers(analysis_type, df_feat_eng, column):\n",
    "\n",
    "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
    "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
    "\n",
    "    if analysis_type == 'numerical':\n",
    "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
    "            df_feat_eng, column)\n",
    "\n",
    "    elif analysis_type == 'outlier_winsorizer':\n",
    "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
    "            df_feat_eng, column)\n",
    "\n",
    "    elif analysis_type == 'onehot_encoder':\n",
    "        df_feat_eng, list_applied_transformers = FeatEngineering_CategoricalEncoder(\n",
    "            df_feat_eng, column)\n",
    "\n",
    "    return df_feat_eng, list_applied_transformers\n",
    "\n",
    "\n",
    "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
    "    # For each variable, assess how the transformations perform\n",
    "    print(f\"* Variable Analyzed: {column}\")\n",
    "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
    "    for col in [column] + list_applied_transformers:\n",
    "\n",
    "        if analysis_type != 'onehot_encoder':\n",
    "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
    "\n",
    "        else:\n",
    "            if col == column:\n",
    "                DiagnosticPlots_Categories(df_feat_eng, col)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.countplot(data=df_feat_eng, x=col, palette=[\n",
    "                  '#432371'], order=df_feat_eng[col].value_counts().index)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def DiagnosticPlots_Numerical(df, variable):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
    "    sns.boxplot(x=df[variable], ax=axes[2])\n",
    "\n",
    "    axes[0].set_title('Histogram')\n",
    "    axes[1].set_title('QQ Plot')\n",
    "    axes[2].set_title('Boxplot')\n",
    "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
    "    list_methods_worked = []\n",
    "    try:\n",
    "        encoder = OneHotEncoder(variables=[f\"{column}_onehot_encoder\"])\n",
    "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
    "        list_methods_worked = [coloh for coloh in df_feat_eng.columns if f\"{column}_onehot_encoder\" in coloh]\n",
    "\n",
    "    except:\n",
    "        df_feat_eng.drop([f\"{column}_onehot_encoder\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feat_eng, list_methods_worked\n",
    "\n",
    "\n",
    "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
    "    list_methods_worked = []\n",
    "\n",
    "    # Winsorizer iqr\n",
    "    try:\n",
    "        disc = Winsorizer(\n",
    "            capping_method='iqr', tail='both', fold=1.5, variables=[f\"{column}_iqr\"])\n",
    "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_iqr\")\n",
    "    except:\n",
    "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feat_eng, list_methods_worked\n",
    "\n",
    "\n",
    "def FeatEngineering_Numerical(df_feat_eng, column):\n",
    "\n",
    "    list_methods_worked = []\n",
    "\n",
    "    # LogTransformer base e\n",
    "    try:\n",
    "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
    "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_log_e\")\n",
    "    except:\n",
    "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
    "\n",
    "        # LogTransformer base 10\n",
    "    try:\n",
    "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
    "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_log_10\")\n",
    "    except:\n",
    "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
    "\n",
    "    # ReciprocalTransformer\n",
    "    try:\n",
    "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
    "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
    "    except:\n",
    "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
    "\n",
    "    # PowerTransformer\n",
    "    try:\n",
    "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
    "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_power\")\n",
    "    except:\n",
    "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
    "\n",
    "    # BoxCoxTransformer\n",
    "    try:\n",
    "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
    "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_box_cox\")\n",
    "    except:\n",
    "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
    "\n",
    "    # YeoJohnsonTransformer\n",
    "    try:\n",
    "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
    "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
    "    except:\n",
    "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feat_eng, list_methods_worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118474d8",
   "metadata": {},
   "source": [
    "### Feature Engineering Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d1dfe",
   "metadata": {},
   "source": [
    "### Categorical Encoding - OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60034475",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_engineering= ['Store', 'Dept', 'Type']\n",
    "variables_engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6928ca4",
   "metadata": {},
   "source": [
    "Create a separate DataFrame, with your variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0593c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_engineering = df[variables_engineering].copy()\n",
    "df_engineering.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c7f31",
   "metadata": {},
   "source": [
    "Create engineered variables(s), apply the transformation(s), assess engineered variables distribution and select the most suitable method for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='onehot_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bf77e",
   "metadata": {},
   "source": [
    "Apply the selected transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44381df",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(variables = variables_engineering)\n",
    "df = encoder.fit_transform(df)\n",
    "\n",
    "print(\"* Categorical encoding - ordinal transformation done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee16946",
   "metadata": {},
   "source": [
    "### Numerical Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21b4f0",
   "metadata": {},
   "source": [
    "Select variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_engineering =  ['Temperature',\n",
    "                        'Fuel_Price',\n",
    "                        'MarkDown1',\n",
    "                        'MarkDown2',\n",
    "                        'MarkDown3',\n",
    "                        'MarkDown4',\n",
    "                        'MarkDown5',\n",
    "                        'CPI',\n",
    "                        'Unemployment',\n",
    "                        'Size'\n",
    "                         ]\n",
    "\n",
    "\n",
    "variables_engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c26630",
   "metadata": {},
   "source": [
    "Create a separate DataFrame, with your variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_engineering = df[variables_engineering].copy()\n",
    "df_engineering.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbf3d0",
   "metadata": {},
   "source": [
    "Create engineered variables(s), apply the transformation(s), assess engineered variables distribution and select the most suitable method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9669c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='numerical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad278ce1",
   "metadata": {},
   "source": [
    "For each variable, our conclusion on how the transformation(s) look(s) to be effective\n",
    "\n",
    "- 'Temperature' - YeoJohnson looks to improve distribution to become more normally distributed\n",
    "\n",
    "- 'MarkDown1' - YeoJohnson looks to improve distribution to become more normally distributed\n",
    "\n",
    "- 'MarkDown2' - YeoJohnson looks to improve distribution to become more normally distributed\n",
    "\n",
    "- 'MarkDown3' - YeoJohnson looks to improve distribution to become more normally distributed\n",
    "\n",
    "- 'MarkDown4' - YeoJohnson looks to improve distribution to become more normally distributed\n",
    "\n",
    "- 'MarkDown5' - YeoJohnson looks to improve distribution to become more normally distributed\n",
    "\n",
    "For the remaining features, the approaches don't look to improve distribution to become more normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28347808",
   "metadata": {},
   "source": [
    "Apply the selected transformation to the Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_engineering_yeo =  ['Temperature', 'MarkDown1',  'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "yeo_transformer = vt.YeoJohnsonTransformer(variables = variables_engineering_yeo)\n",
    "df = yeo_transformer.fit_transform(df)\n",
    "\n",
    "print(\"* Numerical transformation done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a220a",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a4702",
   "metadata": {},
   "source": [
    "Our data is a time series. We can't randomly select rows for train, validation and test samples. We split data by store values. We randomly select 5 stores for validattion set, 9 stores for test set and all rest for training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0514bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_numbers = np.arange(1, 46)\n",
    "np.random.seed(42)\n",
    "validation_stores = np.random.choice(store_numbers, 5, replace=False)\n",
    "remaining_stores = [store for store in store_numbers if store not in validation_stores]\n",
    "test_stores = np.random.choice(remaining_stores, 9, replace=False)\n",
    "train_stores = [store for store in remaining_stores if store not in test_stores]\n",
    "\n",
    "print(f\"Validation Stores: {validation_stores}\")\n",
    "print(f\"Test Stores: {test_stores}\")\n",
    "print(f\"Train Stores: {train_stores}\")\n",
    "\n",
    "#Define a function to filter the dataframe by store columns\n",
    "def filter_by_store(df, store_list):\n",
    "    mask = np.zeros(len(df), dtype=bool)  # Start with a mask of all False\n",
    "    for store in store_list:\n",
    "        mask |= (df[f'Store_{store}'] == 1)  # Set True where the store column equals 1\n",
    "    return df[mask]\n",
    "\n",
    "#Filter the dataset for each of train, validation, and test sets\n",
    "train_data = filter_by_store(df, train_stores)\n",
    "validation_data = filter_by_store(df, validation_stores)\n",
    "test_data = filter_by_store(df, test_stores)\n",
    "\n",
    "#Print out the shape of the datasets to confirm\n",
    "print(f\"Training set size: {train_data.shape}\")\n",
    "print(f\"Validation set size: {validation_data.shape}\")\n",
    "print(f\"Test set size: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f85ea6",
   "metadata": {},
   "source": [
    "### Smart Correlated Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4f982",
   "metadata": {},
   "source": [
    "Create a separate DataFrame, with your variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_engineering = train_data.drop(['Weekly_Sales'], axis=1).copy() \n",
    "# Weekly_Sales is a target variable, but when this transformer is added to the pipeline, it will not use Weekly_Sales in the calculation\n",
    "# but for this analysis here, it would be suitable to drop Weekly_Sales \n",
    "df_engineering.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1657af",
   "metadata": {},
   "source": [
    "Create engineered variables(s) applying the transformation(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6,selection_method=\"variance\")\n",
    "\n",
    "df_selected = corr_sel.fit_transform(df_engineering)\n",
    "corr_sel.correlated_feature_sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sel.features_to_drop_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a75ab7",
   "metadata": {},
   "source": [
    "### Push transformed data to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"outputs/TrainData.csv\"\n",
    "train_data.to_csv(output_path, index=False)\n",
    "\n",
    "output_path = \"outputs/ValidationData.csv\"\n",
    "validation_data.to_csv(output_path, index=False)\n",
    "\n",
    "output_path = \"outputs/TestData.csv\"\n",
    "test_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5ffa0",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selected Features After Smart Correlated Selection:\\n\", df_selected.columns)\n",
    "print(\"Feature engineering process complete. The final set of selected features are ready for modeling.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
